{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Titanic Dataset\n",
    "### In which we explore Disasters, Trees, Classification & the Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Visit Kaggle and download Data from http://www.kaggle.com/c/titanic-gettingStarted\n",
    "2. Read Titanic Data\n",
    "2. Transform and select features\n",
    "3. Create a simple model & Predict\n",
    "4. Submit to Kaggle & checkout the leaderboard\n",
    "5. Decision Tree Model, Predict & Submit\n",
    "6. Random Forest Model, Predict & Submit\n",
    "7. Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run @2015-12-17 21:42:57.865440-08:00\n",
      "Running Spark Version 1.6.0\n",
      "spark.app.name=PySparkShell\n",
      "spark.files=file:/Users/ksankar/.ivy2/jars/com.databricks_spark-csv_2.10-1.3.0.jar,file:/Users/ksankar/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,file:/Users/ksankar/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar\n",
      "spark.jars=file:/Users/ksankar/.ivy2/jars/com.databricks_spark-csv_2.10-1.3.0.jar,file:/Users/ksankar/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,file:/Users/ksankar/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar\n",
      "spark.master=local[*]\n",
      "spark.submit.deployMode=client\n",
      "spark.submit.pyFiles=/Users/ksankar/.ivy2/jars/com.databricks_spark-csv_2.10-1.3.0.jar,/Users/ksankar/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,/Users/ksankar/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pytz import timezone\n",
    "print \"Last run @%s\" % (datetime.datetime.now(timezone('US/Pacific')))\n",
    "#\n",
    "from pyspark.context import SparkContext\n",
    "print \"Running Spark Version %s\" % (sc.version)\n",
    "#\n",
    "from pyspark.conf import SparkConf\n",
    "conf = SparkConf()\n",
    "print conf.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Read Titanic Data\n",
    "### The Data is part of the Kaggle Competition \"Titanic: Machine Learning from Disaster\"\n",
    "### Download data from http://www.kaggle.com/c/titanic-gettingStarted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read Train & Test Datasets\n",
    "train = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('titanic-r/train.csv')\n",
    "test = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('titanic-r/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PassengerId', 'string'),\n",
       " ('Survived', 'string'),\n",
       " ('Pclass', 'string'),\n",
       " ('Name', 'string'),\n",
       " ('Sex', 'string'),\n",
       " ('Age', 'string'),\n",
       " ('SibSp', 'string'),\n",
       " ('Parch', 'string'),\n",
       " ('Ticket', 'string'),\n",
       " ('Fare', 'string'),\n",
       " ('Cabin', 'string'),\n",
       " ('Embarked', 'string')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|summary|\n",
      "+-------+\n",
      "|  count|\n",
      "|   mean|\n",
      "| stddev|\n",
      "|    min|\n",
      "|    max|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+---+-----+-----+---------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|   Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+---------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|A/5 21171|   7.25|     |       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female| 38|    1|    0| PC 17599|71.2833|  C85|       C|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+---------+-------+-----+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "train_1 = train.select(train['PassengerId'], \n",
    "                 train['Survived'].cast(\"integer\").alias(\"Survived\"),\n",
    "                 train['Pclass'].cast(\"integer\").alias(\"Pclass\"),\n",
    "                 F.when(train['Sex'] == 'female', 1).otherwise(0).alias(\"Gender\"), \n",
    "                 train['Age'].cast(\"integer\").alias(\"Age\"),\n",
    "                 train['SibSp'].cast(\"integer\").alias(\"SibSp\"),\n",
    "                 train['Parch'].cast(\"integer\").alias(\"Parch\"),\n",
    "                 train['Fare'].cast(\"float\").alias(\"Fare\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------+---+-----+-----+-------+\n",
      "|PassengerId|Survived|Pclass|Gender|Age|SibSp|Parch|   Fare|\n",
      "+-----------+--------+------+------+---+-----+-----+-------+\n",
      "|          1|       0|     3|     0| 22|    1|    0|   7.25|\n",
      "|          2|       1|     1|     1| 38|    1|    0|71.2833|\n",
      "+-----------+--------+------+------+---+-----+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+-------------------+------------------+------------------+-------------------+-----------------+\n",
      "|summary|           Survived|            Pclass|             Gender|               Age|             SibSp|              Parch|             Fare|\n",
      "+-------+-------------------+------------------+-------------------+------------------+------------------+-------------------+-----------------+\n",
      "|  count|                891|               891|                891|               714|               891|                891|              891|\n",
      "|   mean| 0.3838383838383838| 2.308641975308642|0.35241301907968575|29.712885154061624|0.5230078563411896|0.38159371492704824|32.20420804114722|\n",
      "| stddev|0.48659245426485737|0.8360712409770491| 0.4779900708960981|14.529273128376586| 1.102743432293432| 0.8060572211299486|49.69342916316157|\n",
      "|    min|                  0|                 1|                  0|                 0|                 0|                  0|              0.0|\n",
      "|    max|                  1|                 3|                  1|                80|                 8|                  6|         512.3292|\n",
      "+-------+-------------------+------------------+-------------------+------------------+------------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_1.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------+----+-----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Gender| Age|SibSp|Parch|    Fare|\n",
      "+-----------+--------+------+------+----+-----+-----+--------+\n",
      "|          6|       0|     3|     0|null|    0|    0|  8.4583|\n",
      "|         18|       1|     2|     0|null|    0|    0|    13.0|\n",
      "|         20|       1|     3|     1|null|    0|    0|   7.225|\n",
      "|         27|       0|     3|     0|null|    0|    0|   7.225|\n",
      "|         29|       1|     3|     1|null|    0|    0|  7.8792|\n",
      "|         30|       0|     3|     0|null|    0|    0|  7.8958|\n",
      "|         32|       1|     1|     1|null|    1|    0|146.5208|\n",
      "|         33|       1|     3|     1|null|    0|    0|    7.75|\n",
      "|         37|       1|     3|     0|null|    0|    0|  7.2292|\n",
      "|         43|       0|     3|     0|null|    0|    0|  7.8958|\n",
      "|         46|       0|     3|     0|null|    0|    0|    8.05|\n",
      "|         47|       0|     3|     0|null|    1|    0|    15.5|\n",
      "|         48|       1|     3|     1|null|    0|    0|    7.75|\n",
      "|         49|       0|     3|     0|null|    2|    0| 21.6792|\n",
      "|         56|       1|     1|     0|null|    0|    0|    35.5|\n",
      "|         65|       0|     1|     0|null|    0|    0| 27.7208|\n",
      "|         66|       1|     3|     0|null|    1|    1| 15.2458|\n",
      "|         77|       0|     3|     0|null|    0|    0|  7.8958|\n",
      "|         78|       0|     3|     0|null|    0|    0|    8.05|\n",
      "|         83|       1|     3|     1|null|    0|    0|  7.7875|\n",
      "|         88|       0|     3|     0|null|    0|    0|    8.05|\n",
      "|         96|       0|     3|     0|null|    0|    0|    8.05|\n",
      "|        102|       0|     3|     0|null|    0|    0|  7.8958|\n",
      "|        108|       1|     3|     0|null|    0|    0|   7.775|\n",
      "|        110|       1|     3|     1|null|    1|    0|   24.15|\n",
      "|        122|       0|     3|     0|null|    0|    0|    8.05|\n",
      "|        127|       0|     3|     0|null|    0|    0|    7.75|\n",
      "|        129|       1|     3|     1|null|    1|    1| 22.3583|\n",
      "|        141|       0|     3|     1|null|    0|    2| 15.2458|\n",
      "|        155|       0|     3|     0|null|    0|    0|  7.3125|\n",
      "|        159|       0|     3|     0|null|    0|    0|  8.6625|\n",
      "|        160|       0|     3|     0|null|    8|    2|   69.55|\n",
      "|        167|       1|     1|     1|null|    0|    1|    55.0|\n",
      "|        169|       0|     1|     0|null|    0|    0|  25.925|\n",
      "|        177|       0|     3|     0|null|    3|    1| 25.4667|\n",
      "|        181|       0|     3|     1|null|    8|    2|   69.55|\n",
      "|        182|       0|     2|     0|null|    0|    0|   15.05|\n",
      "|        186|       0|     1|     0|null|    0|    0|    50.0|\n",
      "|        187|       1|     3|     1|null|    1|    0|    15.5|\n",
      "|        197|       0|     3|     0|null|    0|    0|    7.75|\n",
      "+-----------+--------+------+------+----+-----+-----+--------+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace null age by 30\n",
    "# Do we have nulls ?\n",
    "train_1.filter(train_1['Age'].isNull()).show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------+---+-----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Gender|Age|SibSp|Parch|    Fare|\n",
      "+-----------+--------+------+------+---+-----+-----+--------+\n",
      "|          1|       0|     3|     0| 22|    1|    0|    7.25|\n",
      "|          2|       1|     1|     1| 38|    1|    0| 71.2833|\n",
      "|          3|       1|     3|     1| 26|    0|    0|   7.925|\n",
      "|          4|       1|     1|     1| 35|    1|    0|    53.1|\n",
      "|          5|       0|     3|     0| 35|    0|    0|    8.05|\n",
      "|          6|       0|     3|     0| 30|    0|    0|  8.4583|\n",
      "|          7|       0|     1|     0| 54|    0|    0| 51.8625|\n",
      "|          8|       0|     3|     0|  2|    3|    1|  21.075|\n",
      "|          9|       1|     3|     1| 27|    0|    2| 11.1333|\n",
      "|         10|       1|     2|     1| 14|    1|    0| 30.0708|\n",
      "|         11|       1|     3|     1|  4|    1|    1|    16.7|\n",
      "|         12|       1|     1|     1| 58|    0|    0|   26.55|\n",
      "|         13|       0|     3|     0| 20|    0|    0|    8.05|\n",
      "|         14|       0|     3|     0| 39|    1|    5|  31.275|\n",
      "|         15|       0|     3|     1| 14|    0|    0|  7.8542|\n",
      "|         16|       1|     2|     1| 55|    0|    0|    16.0|\n",
      "|         17|       0|     3|     0|  2|    4|    1|  29.125|\n",
      "|         18|       1|     2|     0| 30|    0|    0|    13.0|\n",
      "|         19|       0|     3|     1| 31|    1|    0|    18.0|\n",
      "|         20|       1|     3|     1| 30|    0|    0|   7.225|\n",
      "|         21|       0|     2|     0| 35|    0|    0|    26.0|\n",
      "|         22|       1|     2|     0| 34|    0|    0|    13.0|\n",
      "|         23|       1|     3|     1| 15|    0|    0|  8.0292|\n",
      "|         24|       1|     1|     0| 28|    0|    0|    35.5|\n",
      "|         25|       0|     3|     1|  8|    3|    1|  21.075|\n",
      "|         26|       1|     3|     1| 38|    1|    5| 31.3875|\n",
      "|         27|       0|     3|     0| 30|    0|    0|   7.225|\n",
      "|         28|       0|     1|     0| 19|    3|    2|   263.0|\n",
      "|         29|       1|     3|     1| 30|    0|    0|  7.8792|\n",
      "|         30|       0|     3|     0| 30|    0|    0|  7.8958|\n",
      "|         31|       0|     1|     0| 40|    0|    0| 27.7208|\n",
      "|         32|       1|     1|     1| 30|    1|    0|146.5208|\n",
      "|         33|       1|     3|     1| 30|    0|    0|    7.75|\n",
      "|         34|       0|     2|     0| 66|    0|    0|    10.5|\n",
      "|         35|       0|     1|     0| 28|    1|    0| 82.1708|\n",
      "|         36|       0|     1|     0| 42|    1|    0|    52.0|\n",
      "|         37|       1|     3|     0| 30|    0|    0|  7.2292|\n",
      "|         38|       0|     3|     0| 21|    0|    0|    8.05|\n",
      "|         39|       0|     3|     1| 18|    2|    0|    18.0|\n",
      "|         40|       1|     3|     1| 14|    1|    0| 11.2417|\n",
      "+-----------+--------+------+------+---+-----+-----+--------+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace null age by 30\n",
    "train_1.na.fill(30,'Age').show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace null age by 30\n",
    "train_2 = train_1.na.fill(30,'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+---+\n",
      "|Gender_Survived|  0|  1|\n",
      "+---------------+---+---+\n",
      "|              1| 81|233|\n",
      "|              0|468|109|\n",
      "+---------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_2.crosstab(\"Gender\",\"Survived\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F = 74.20% M = 18.89%\n"
     ]
    }
   ],
   "source": [
    "print \"F = %3.2f%% M = %3.2f%%\" % ( (100*233.0/(233+81)), (100*109.0/(109+468)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dick, The butcher to Jack Cade\n",
    "### Dick: The first thing we do, let's kill all the men.\n",
    "### Cade: Nay, that I mean to do.\n",
    "#### Ref : http://www.enotes.com/shakespeare-quotes/lets-kill-all-lawyers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------------+------+----+-----+-----+------+------+-----+--------+\n",
      "|PassengerId|Pclass|                Name|   Sex| Age|SibSp|Parch|Ticket|  Fare|Cabin|Embarked|\n",
      "+-----------+------+--------------------+------+----+-----+-----+------+------+-----+--------+\n",
      "|        892|     3|    Kelly, Mr. James|  male|34.5|    0|    0|330911|7.8292|     |       Q|\n",
      "|        893|     3|Wilkes, Mrs. Jame...|female|  47|    1|    0|363272|     7|     |       S|\n",
      "+-----------+------+--------------------+------+----+-----+-----+------+------+-----+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# 1 : Simple Model (M=Survived) \n",
    "#\n",
    "test.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = test.select(test['PassengerId'], \n",
    "                 F.when(test['Sex'] == 'female', 1).otherwise(0).alias(\"Survived\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|PassengerId|Survived|\n",
      "+-----------+--------+\n",
      "|        892|       0|\n",
      "|        893|       1|\n",
      "+-----------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out.coalesce(1).write.mode('overwrite').format('com.databricks.spark.csv')\\\n",
    ".options(header='true').save('titanic-r/spark-sub-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Submit\n",
    "# Rank : 2586 Score : 0.76555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+---+\n",
      "|Age_Survived|  1|  0|\n",
      "+------------+---+---+\n",
      "|           0|  1|  0|\n",
      "|           5|  4|  0|\n",
      "|          10|  0|  2|\n",
      "|          56|  2|  3|\n",
      "|          42|  6|  7|\n",
      "|          24| 15| 16|\n",
      "|          37|  1|  6|\n",
      "|          25|  6| 18|\n",
      "|          52|  3|  3|\n",
      "|          14|  3|  3|\n",
      "|          20|  3| 12|\n",
      "|          46|  0|  5|\n",
      "|          57|  0|  2|\n",
      "|          29|  8| 14|\n",
      "|          61|  0|  3|\n",
      "|           1| 11|  2|\n",
      "|          74|  0|  1|\n",
      "|           6|  2|  1|\n",
      "|          60|  2|  2|\n",
      "|          28|  7| 18|\n",
      "+------------+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Would age be a better predictor ?\n",
    "#\n",
    "train_1.na.drop().crosstab(\"Age\",\"Survived\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# *** Home work : See if Pclass, SibSp or Parch is a better indication and change survival accordingly¶\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "def parse_passenger_list(r):\n",
    "    return LabeledPoint(r[1],[r[2],r[3],r[4],r[5],r[6],r[7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_rdd = train_2.map(lambda x: parse_passenger_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(0.0, [3.0,0.0,22.0,1.0,0.0,7.25])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "model = DecisionTree.trainClassifier(train_rdd, numClasses=2,categoricalFeaturesInfo={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeModel classifier of depth 5 with 49 nodes\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "# print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform test and predict\n",
    "import pyspark.sql.functions as F\n",
    "test_1 = test.select(test['PassengerId'], \n",
    "                 test['Pclass'].cast(\"integer\").alias(\"Pclass\"),\n",
    "                 F.when(test['Sex'] == 'female', 1).otherwise(0).alias(\"Gender\"), \n",
    "                 test['Age'].cast(\"integer\").alias(\"Age\"),\n",
    "                 test['SibSp'].cast(\"integer\").alias(\"SibSp\"),\n",
    "                 test['Parch'].cast(\"integer\").alias(\"Parch\"),\n",
    "                 test['Fare'].cast(\"float\").alias(\"Fare\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+---+-----+-----+------+\n",
      "|PassengerId|Pclass|Gender|Age|SibSp|Parch|  Fare|\n",
      "+-----------+------+------+---+-----+-----+------+\n",
      "|        892|     3|     0| 35|    0|    0|7.8292|\n",
      "|        893|     3|     1| 47|    1|    0|   7.0|\n",
      "+-----------+------+------+---+-----+-----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+----+-----+-----+-------+\n",
      "|PassengerId|Pclass|Gender| Age|SibSp|Parch|   Fare|\n",
      "+-----------+------+------+----+-----+-----+-------+\n",
      "|        902|     3|     0|null|    0|    0| 7.8958|\n",
      "|        914|     1|     1|null|    0|    0|31.6833|\n",
      "|        921|     3|     0|null|    2|    0|21.6792|\n",
      "|        925|     3|     1|null|    1|    2|  23.45|\n",
      "|        928|     3|     1|null|    0|    0|   8.05|\n",
      "|        931|     3|     0|null|    0|    0|56.4958|\n",
      "|        933|     1|     0|null|    0|    0|  26.55|\n",
      "|        939|     3|     0|null|    0|    0|   7.75|\n",
      "|        946|     2|     0|null|    0|    0|15.5792|\n",
      "|        950|     3|     0|null|    1|    0|   16.1|\n",
      "|        957|     2|     1|null|    0|    0|   21.0|\n",
      "|        968|     3|     0|null|    0|    0|   8.05|\n",
      "|        975|     3|     0|null|    0|    0| 7.8958|\n",
      "|        976|     2|     0|null|    0|    0|10.7083|\n",
      "|        977|     3|     0|null|    1|    0|14.4542|\n",
      "|        980|     3|     1|null|    0|    0|   7.75|\n",
      "|        983|     3|     0|null|    0|    0|  7.775|\n",
      "|        985|     3|     0|null|    0|    0|   8.05|\n",
      "|        994|     3|     0|null|    0|    0|   7.75|\n",
      "|        999|     3|     0|null|    0|    0|   7.75|\n",
      "|       1000|     3|     0|null|    0|    0| 8.7125|\n",
      "|       1003|     3|     1|null|    0|    0| 7.7792|\n",
      "|       1008|     3|     0|null|    0|    0| 6.4375|\n",
      "|       1013|     3|     0|null|    1|    0|   7.75|\n",
      "|       1016|     3|     0|null|    0|    0|   7.75|\n",
      "|       1019|     3|     1|null|    2|    0|  23.25|\n",
      "|       1024|     3|     1|null|    0|    4|25.4667|\n",
      "|       1025|     3|     0|null|    1|    0| 6.4375|\n",
      "|       1038|     1|     0|null|    0|    0|51.8625|\n",
      "|       1040|     1|     0|null|    0|    0|  26.55|\n",
      "|       1043|     3|     0|null|    0|    0| 7.8958|\n",
      "|       1052|     3|     1|null|    0|    0| 7.7333|\n",
      "|       1055|     3|     0|null|    0|    0|    7.0|\n",
      "|       1060|     1|     1|null|    0|    0|27.7208|\n",
      "|       1062|     3|     0|null|    0|    0|   7.55|\n",
      "|       1065|     3|     0|null|    0|    0| 7.2292|\n",
      "|       1075|     3|     0|null|    0|    0|   7.75|\n",
      "|       1080|     3|     1|null|    8|    2|  69.55|\n",
      "|       1083|     1|     0|null|    0|    0|   26.0|\n",
      "|       1091|     3|     1|null|    0|    0| 8.1125|\n",
      "+-----------+------+------+----+-----+-----+-------+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do we have nulls ?\n",
    "test_1.filter(test_1['Age'].isNull()).show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          avg(Age)|\n",
      "+------------------+\n",
      "|30.295180722891565|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_1.groupBy().avg('Age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace null age by 30.24 - the mean\n",
    "test_2 = test_1.na.fill(30,'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parse test data for predictions\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "def parse_test(r):\n",
    "    return (r[1],r[2],r[3],r[4],r[5],r[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_rdd = test_2.map(lambda x: parse_test(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_rdd = test_2.map(lambda x: x[0]).zip(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'892', 0.0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_df = out_rdd.toDF(['PassengerId','Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|PassengerId|Survived|\n",
      "+-----------+--------+\n",
      "|        892|     0.0|\n",
      "|        893|     0.0|\n",
      "+-----------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_1 = out_df.select(out_df['PassengerId'],\n",
    "                      out_df['Survived'].cast('integer').alias('Survived'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|PassengerId|Survived|\n",
      "+-----------+--------+\n",
      "|        892|       0|\n",
      "|        893|       0|\n",
      "+-----------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out_1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_1.coalesce(1).write.mode('overwrite').format('com.databricks.spark.csv')\\\n",
    ".options(header='true').save('titanic-r/spark-sub-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Submit\n",
    "# Rank : 2038 +549 Score : 0.77512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "model_rf = RandomForest.trainClassifier(train_rdd, numClasses=2,categoricalFeaturesInfo={},numTrees=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeEnsembleModel classifier with 42 trees\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_rf)\n",
    "#print(model_rf.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_rf = model_rf.predict(test_rdd).coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_rf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_rf = test_2.map(lambda x: x[0]).coalesce(1).zip(pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'892', 0.0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_rf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_df_rf = out_rf.toDF(['PassengerId','Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_2 = out_df_rf.select(out_df_rf['PassengerId'],\n",
    "                      out_df_rf['Survived'].cast('integer').alias('Survived'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_2.coalesce(1).write.mode('overwrite').format('com.databricks.spark.csv')\\\n",
    ".options(header='true').save('titanic-r/spark-sub-03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Submit\n",
    "# Rank : 1550 +488 Score : 0.78469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looks like we are on a roll ! Let us try SVM !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "model_svm = SVMWithSGD.train(train_rdd, iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_svm = model_svm.predict(test_rdd).coalesce(1)\n",
    "out_svm = test_2.map(lambda x: x[0]).coalesce(1).zip(pred_svm)\n",
    "out_df_svm = out_svm.toDF(['PassengerId','Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_3 = out_df_svm.select(out_df_svm['PassengerId'],\n",
    "                      out_df_svm['Survived'].cast('integer').alias('Survived'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_3.coalesce(1).write.mode('overwrite').format('com.databricks.spark.csv')\\\n",
    ".options(header='true').save('titanic-r/spark-sub-04.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not good. Only 0.39713 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Did Random Forest or SVM do Better ? \n",
    "#### Why ? Why Not ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Science Folk Wisdom\n",
    "http://www.slideshare.net/ksankar/data-science-folk-knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
