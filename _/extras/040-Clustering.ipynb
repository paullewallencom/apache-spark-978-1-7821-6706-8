{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "###In which we explore Segmenting Frequent InterGallacticHoppers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "\n",
    "* ###InterGallactic Airlines have the GallacticHoppers frequent flyer program & have data about their customers who participate in the program. \n",
    "* ###The airlines execs have a feeling that other airlines will poach their customers if they do not keep their loyal customers happy.\n",
    "* ###So the business want to customize promotions to their frequent flier program.\n",
    "* ###Can they just have one type of promotion ? \n",
    "* ###Should they have different types of incentives ?\n",
    "* ###Who exactly are the customers in their GallacticHoppers program ?\n",
    "* ###Recently they have deployed an infrastructure with Spark\n",
    "* ###Can Spark help in this business problem ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run @2015-07-15 15:18:08.185957-07:00\n",
      "Running Spark Version 1.4.1\n",
      "spark.app.name=pyspark-shell\n",
      "spark.files=file:/Users/ksankar/.ivy2/jars/com.databricks_spark-csv_2.11-1.0.3.jar,file:/Users/ksankar/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar\n",
      "spark.jars=file:/Users/ksankar/.ivy2/jars/com.databricks_spark-csv_2.11-1.0.3.jar,file:/Users/ksankar/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar\n",
      "spark.master=local[*]\n",
      "spark.submit.pyFiles=/Users/ksankar/.ivy2/jars/com.databricks_spark-csv_2.11-1.0.3.jar,/Users/ksankar/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pytz import timezone\n",
    "print \"Last run @%s\" % (datetime.datetime.now(timezone('US/Pacific')))\n",
    "#\n",
    "from pyspark.context import SparkContext\n",
    "print \"Running Spark Version %s\" % (sc.version)\n",
    "#\n",
    "from pyspark.conf import SparkConf\n",
    "conf = SparkConf()\n",
    "print conf.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read Dataset\n",
    "freq_df = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "            .options(header='true')\\\n",
    "            .load('freq-flyer/AirlinesCluster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+----------+-----------+-----------+---------------+\n",
      "|Balance|QualMiles|BonusMiles|BonusTrans|FlightMiles|FlightTrans|DaysSinceEnroll|\n",
      "+-------+---------+----------+----------+-----------+-----------+---------------+\n",
      "|  28143|        0|       174|         1|          0|          0|           7000|\n",
      "|  19244|        0|       215|         2|          0|          0|           6968|\n",
      "|  41354|        0|      4123|         4|          0|          0|           7034|\n",
      "|  14776|        0|       500|         1|          0|          0|           6952|\n",
      "|  97752|        0|     43300|        26|       2077|          4|           6935|\n",
      "+-------+---------+----------+----------+-----------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freq_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3999"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Balance', 'string'),\n",
       " ('QualMiles', 'string'),\n",
       " ('BonusMiles', 'string'),\n",
       " ('BonusTrans', 'string'),\n",
       " ('FlightMiles', 'string'),\n",
       " ('FlightTrans', 'string'),\n",
       " ('DaysSinceEnroll', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### But we need a table of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "freq_rdd = freq_df.map(lambda row: array([float(x) for x in row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  2.81430000e+04,   0.00000000e+00,   1.74000000e+02,\n",
       "          1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          7.00000000e+03]),\n",
       " array([  1.92440000e+04,   0.00000000e+00,   2.15000000e+02,\n",
       "          2.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          6.96800000e+03]),\n",
       " array([  4.13540000e+04,   0.00000000e+00,   4.12300000e+03,\n",
       "          4.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          7.03400000e+03])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.81430000e+04,   0.00000000e+00,   1.74000000e+02,\n",
       "         1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         7.00000000e+03])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_rdd.first()\n",
    "# Balance, TopStatusQualMiles, NonFlightMiles, NonFlightTrans, FlightMiles, FlightTrans, DaysSinceEnroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method train in module pyspark.mllib.clustering:\n",
      "\n",
      "train(cls, rdd, k, maxIterations=100, runs=1, initializationMode='k-means||', seed=None, initializationSteps=5, epsilon=0.0001) method of __builtin__.type instance\n",
      "    Train a k-means clustering model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(KMeans.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "km_mdl_1 = KMeans.train(freq_rdd, 2, maxIterations=10,runs=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 51356.287    126.293  14976.641     10.936    389.559      1.151   4001.617\n",
      "320111.326    341.607  41171.964     18.976   1241.275      3.843   5414.462\n"
     ]
    }
   ],
   "source": [
    "for x in km_mdl_1.clusterCenters:\n",
    "        print \"%10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (x[0],x[1],x[2],x[3],x[4],x[5],x[6])\n",
    "# Balance, TopStatusQualMiles, NonFlightMiles, NonFlightTrans, FlightMiles, FlightTrans, DaysSinceEnroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.81430000e+04   0.00000000e+00   1.74000000e+02   1.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   7.00000000e+03] 0\n",
      "[  1.92440000e+04   0.00000000e+00   2.15000000e+02   2.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   6.96800000e+03] 0\n",
      "[  4.13540000e+04   0.00000000e+00   4.12300000e+03   4.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   7.03400000e+03] 0\n",
      "[  1.47760000e+04   0.00000000e+00   5.00000000e+02   1.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   6.95200000e+03] 0\n",
      "[  9.77520000e+04   0.00000000e+00   4.33000000e+04   2.60000000e+01\n",
      "   2.07700000e+03   4.00000000e+00   6.93500000e+03] 0\n",
      "[ 16420.      0.      0.      0.      0.      0.   6942.] 0\n",
      "[  8.49140000e+04   0.00000000e+00   2.74820000e+04   2.50000000e+01\n",
      "   0.00000000e+00   0.00000000e+00   6.99400000e+03] 0\n",
      "[  2.08560000e+04   0.00000000e+00   5.25000000e+03   4.00000000e+00\n",
      "   2.50000000e+02   1.00000000e+00   6.93800000e+03] 0\n",
      "[  4.43003000e+05   0.00000000e+00   1.75300000e+03   4.30000000e+01\n",
      "   3.85000000e+03   1.20000000e+01   6.94800000e+03] 1\n",
      "[  1.04860000e+05   0.00000000e+00   2.84260000e+04   2.80000000e+01\n",
      "   1.15000000e+03   3.00000000e+00   6.93100000e+03] 0\n"
     ]
    }
   ],
   "source": [
    "for x in freq_rdd.take(10):\n",
    "    print x,km_mdl_1.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def squared_error(mdl, point):\n",
    "    center = mdl.centers[mdl.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Error = 191906529.055\n"
     ]
    }
   ],
   "source": [
    "WSSSE = freq_rdd.map(lambda point: squared_error(km_mdl_1,point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.36013276e+04   1.44114529e+02   1.71448462e+04   1.16019005e+01\n",
      "   4.60055764e+02   1.37359340e+00   4.11855939e+03]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "summary = Statistics.colStats(freq_rdd)\n",
    "print summary.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean :  73601.328    144.115  17144.846     11.602    460.056      1.374   4118.559\n",
      "Max  : 1704838.000  11148.000 263685.000     86.000  30817.000     53.000   8296.000\n",
      "Min  :      0.000      0.000      0.000      0.000      0.000      0.000      2.000\n",
      "Variance : 10155734647.781 598555.682 583269246.943     92.233 1960585.724     14.388 4264780.669\n"
     ]
    }
   ],
   "source": [
    "print \"Mean : %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (summary.mean()[0],summary.mean()[1],summary.mean()[2],\n",
    "                                                            summary.mean()[3],summary.mean()[4],summary.mean()[5],\n",
    "                                                            summary.mean()[6])\n",
    "print \"Max  : %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (summary.max()[0],summary.max()[1],\n",
    "                                                                       summary.max()[2],\n",
    "                                                            summary.max()[3],summary.max()[4],summary.max()[5],\n",
    "                                                            summary.max()[6])\n",
    "print \"Min  : %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (summary.min()[0],summary.min()[1],\n",
    "                                                                       summary.min()[2],\n",
    "                                                            summary.min()[3],summary.min()[4],summary.min()[5],\n",
    "                                                            summary.min()[6])\n",
    "print \"Variance : %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (summary.variance()[0],summary.variance()[1],\n",
    "                                                                       summary.variance()[2],\n",
    "                                                            summary.variance()[3],summary.variance()[4],summary.variance()[5],\n",
    "                                                            summary.variance()[6])\n",
    "# Balance, TopStatusQualMiles, NonFlightMiles, NonFlightTrans, FlightMiles, FlightTrans, DaysSinceEnroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You see, K-means clustering is \"isotropic\" in all directions of space and therefore tends to produce \n",
    "# more or less round (rather than elongated) clusters. [Ref 2]\n",
    "# In this situation leaving variances unequal is equivalent to putting more weight on variables with smaller variance, \n",
    "# so clusters will tend to be separated along variables with greater variance. [Ref 3]\n",
    "#\n",
    "# center, scale, box-cox, preprocess in caret\n",
    "# zero mean and unit variance\n",
    "#\n",
    "# (x - mu)/sigma\n",
    "# org.apache.spark.mlib.feature.StandardScaler does this, but to the best of my knowledge \n",
    "#            as of now (9/28/14) not available for python \n",
    "# So we do it manually, gives us a chance to do some functional programming !\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_mean = summary.mean()\n",
    "data_sigma = summary.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10155734647.8 100775.664958\n",
      "598555.682228 773.663804393\n",
      "583269246.943 24150.9678262\n",
      "92.2331729756 9.6038103363\n",
      "1960585.7235 1400.20917134\n",
      "14.3881569442 3.79317241161\n",
      "4264780.66925 2065.13454023\n"
     ]
    }
   ],
   "source": [
    "for x in data_sigma:\n",
    "    print x,sqrt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def center_and_scale(a_record):\n",
    "    for i in range(len(a_record)):\n",
    "        a_record[i] = (a_record[i] - data_mean[i])/sqrt(data_sigma[i]) # (x-mean)/sd\n",
    "    return a_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_norm_rdd = freq_rdd.map(lambda x: center_and_scale(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.45108437, -0.18627539, -0.70269839, -1.10392647, -0.32856217,\n",
       "       -0.36212258,  1.39527985])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_norm_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now let us try with the standardized data\n",
    "km_mdl_std = KMeans.train(freq_norm_rdd, 2, maxIterations=10,runs=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1.422      0.733      1.363      1.461      1.761      1.918      0.460\n",
      "    -0.158     -0.081     -0.151     -0.162     -0.195     -0.213     -0.051\n"
     ]
    }
   ],
   "source": [
    "for x in km_mdl_std.clusterCenters:\n",
    "        print \"%10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (x[0],x[1],x[2],x[3],x[4],x[5],x[6])\n",
    "# Balance, TopStatusQualMiles, NonFlightMiles, NonFlightTrans, FlightMiles, FlightTrans, DaysSinceEnroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Error = 7575.5729814\n"
     ]
    }
   ],
   "source": [
    "WSSSE = freq_norm_rdd.map(lambda point: squared_error(km_mdl_std,point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let us try with k= 5 clusters instead of k=2\n",
    "km_mdl_std_5 = KMeans.train(freq_norm_rdd, 5, maxIterations=10,runs=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.425      6.683      0.120      0.115      0.370      0.416     -0.087\n",
      "    -0.381     -0.142     -0.488     -0.510     -0.204     -0.220     -0.840\n",
      "     0.581     -0.112      1.188      0.940     -0.060     -0.071      0.212\n",
      "    -0.117     -0.116     -0.354     -0.266     -0.184     -0.195      0.921\n",
      "     1.229      0.358      0.869      1.738      3.407      3.679      0.289\n"
     ]
    }
   ],
   "source": [
    "for x in km_mdl_std_5.clusterCenters:\n",
    "        print \"%10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (x[0],x[1],x[2],x[3],x[4],x[5],x[6])\n",
    "# Balance, TopStatusQualMiles, NonFlightMiles, NonFlightTrans, FlightMiles, FlightTrans, DaysSinceEnroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Error = 5788.536169\n"
     ]
    }
   ],
   "source": [
    "WSSSE = freq_norm_rdd.map(lambda point: squared_error(km_mdl_std_5,point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.882     -0.037      2.737      1.536      0.150      0.200      0.382\n",
      "     5.178      0.185      1.160      0.748      0.493      0.772      1.035\n",
      "    -0.332     -0.156     -0.602     -0.843     -0.234     -0.234     -0.172\n",
      "    -0.120     -0.139      0.126      0.575     -0.241     -0.258     -0.610\n",
      "    -0.273     -0.098     -0.536     -0.587     -0.228     -0.237      1.039\n",
      "    -0.475     -0.112     -0.596     -0.804     -0.245     -0.256     -1.276\n",
      "     0.193      0.055      0.064      0.697      1.863      1.837      0.010\n",
      "     0.317     -0.132      0.587      0.588     -0.188     -0.210      1.003\n",
      "     0.413      7.036      0.099      0.094      0.353      0.391     -0.092\n",
      "     0.759      0.480      0.973      2.533      5.742      6.047      0.165\n",
      "Within Set Sum of Squared Error = 4709.48068684\n"
     ]
    }
   ],
   "source": [
    "km_mdl_std_10 = KMeans.train(freq_norm_rdd, 10, maxIterations=10,runs=10, initializationMode=\"random\")\n",
    "for x in km_mdl_std_10.clusterCenters:\n",
    "        print \"%10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (x[0],x[1],x[2],x[3],x[4],x[5],x[6])\n",
    "#\n",
    "WSSSE = freq_norm_rdd.map(lambda point: squared_error(km_mdl_std_10,point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_rdd = freq_norm_rdd.map(lambda x: km_mdl_std_5.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3, 3, 2, 3, 2, 3, 4, 2]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([28143,     0,   174,     1,     0,     0,  7000]), 3),\n",
       " (array([19244,     0,   215,     2,     0,     0,  6968]), 3),\n",
       " (array([41354,     0,  4123,     4,     0,     0,  7034]), 3),\n",
       " (array([14776,     0,   500,     1,     0,     0,  6952]), 3),\n",
       " (array([97752,     0, 43300,    26,  2077,     4,  6935]), 2)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_rdd_1 = inp_file.map(lambda line: array([int(x) for x in line.split(',')]))\n",
    "freq_cluster_map = freq_rdd_1.zip(cluster_rdd)\n",
    "freq_cluster_map.take(5) \n",
    "# Gives org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([28143,     0,   174,     1,     0,     0,  7000]), 3),\n",
       " (array([19244,     0,   215,     2,     0,     0,  6968]), 3),\n",
       " (array([41354,     0,  4123,     4,     0,     0,  7034]), 3),\n",
       " (array([14776,     0,   500,     1,     0,     0,  6952]), 3),\n",
       " (array([97752,     0, 43300,    26,  2077,     4,  6935]), 2)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_cluster_map = inp_file.map(lambda line: array([int(x) for x in line.split(',')])).zip(cluster_rdd)\n",
    "freq_cluster_map.take(5) \n",
    "# Gives org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([  2.81430000e+04,   0.00000000e+00,   1.74000000e+02,\n",
       "           1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "           7.00000000e+03]), 3),\n",
       " (array([  1.92440000e+04,   0.00000000e+00,   2.15000000e+02,\n",
       "           2.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "           6.96800000e+03]), 3),\n",
       " (array([  4.13540000e+04,   0.00000000e+00,   4.12300000e+03,\n",
       "           4.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "           7.03400000e+03]), 3),\n",
       " (array([  1.47760000e+04,   0.00000000e+00,   5.00000000e+02,\n",
       "           1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "           6.95200000e+03]), 3),\n",
       " (array([  9.77520000e+04,   0.00000000e+00,   4.33000000e+04,\n",
       "           2.60000000e+01,   2.07700000e+03,   4.00000000e+00,\n",
       "           6.93500000e+03]), 2)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_cluster_map = freq_rdd.zip(cluster_rdd)\n",
    "freq_cluster_map.take(5) # This works !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_0 = freq_cluster_map.filter(lambda x: x[1] == 0)\n",
    "cluster_1 = freq_cluster_map.filter(lambda x: x[1] == 1)\n",
    "cluster_2 = freq_cluster_map.filter(lambda x: x[1] == 2)\n",
    "cluster_3 = freq_cluster_map.filter(lambda x: x[1] == 3)\n",
    "cluster_4 = freq_cluster_map.filter(lambda x: x[1] == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "1639\n",
      "877\n",
      "1263\n",
      "159\n"
     ]
    }
   ],
   "source": [
    "print cluster_0.count()\n",
    "print cluster_1.count()\n",
    "print cluster_2.count()\n",
    "print cluster_3.count()\n",
    "print cluster_4.count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "143\n",
    "1372\n",
    "768\n",
    "59\n",
    "1657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3999"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_0.count()+cluster_1.count()+cluster_2.count()+cluster_3.count()+cluster_4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3999"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_rdd_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3999"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_cluster_map.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([  8.44090000e+04,   5.03100000e+03,   1.54360000e+04,\n",
       "           1.60000000e+01,   1.15000000e+03,   4.00000000e+00,\n",
       "           7.76600000e+03]), 0),\n",
       " (array([  2.78457000e+05,   6.72700000e+03,   5.73130000e+04,\n",
       "           2.70000000e+01,   1.00000000e+03,   2.00000000e+00,\n",
       "           7.10100000e+03]), 0),\n",
       " (array([  5.29886000e+05,   7.21000000e+03,   2.38660000e+04,\n",
       "           2.60000000e+01,   7.74100000e+03,   1.50000000e+01,\n",
       "           8.29600000e+03]), 0),\n",
       " (array([  8.65200000e+04,   3.44500000e+03,   6.44500000e+04,\n",
       "           2.00000000e+01,   1.00000000e+03,   2.00000000e+00,\n",
       "           6.59200000e+03]), 0),\n",
       " (array([  1.33445000e+05,   8.26400000e+03,   3.37500000e+03,\n",
       "           1.30000000e+01,   0.00000000e+00,   0.00000000e+00,\n",
       "           6.49200000e+03]), 0)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_0.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 1625.,     0.,  1375.,     4.,     0.,     0.,  1547.]), 1),\n",
       " (array([  2.46980000e+04,   0.00000000e+00,   1.32900000e+03,\n",
       "           5.00000000e+00,   5.00000000e+02,   1.00000000e+00,\n",
       "           4.26700000e+03]), 1),\n",
       " (array([  3.14840000e+04,   0.00000000e+00,   3.12500000e+03,\n",
       "           1.10000000e+01,   0.00000000e+00,   0.00000000e+00,\n",
       "           4.11700000e+03]), 1),\n",
       " (array([  2.20930000e+04,   0.00000000e+00,   1.48570000e+04,\n",
       "           1.10000000e+01,   2.00000000e+02,   1.00000000e+00,\n",
       "           2.58700000e+03]), 1),\n",
       " (array([  4.46650000e+04,   0.00000000e+00,   3.33000000e+02,\n",
       "           2.00000000e+00,   3.33000000e+02,   2.00000000e+00,\n",
       "           3.60100000e+03]), 1)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([  9.77520000e+04,   0.00000000e+00,   4.33000000e+04,\n",
       "           2.60000000e+01,   2.07700000e+03,   4.00000000e+00,\n",
       "           6.93500000e+03]), 2),\n",
       " (array([  8.49140000e+04,   0.00000000e+00,   2.74820000e+04,\n",
       "           2.50000000e+01,   0.00000000e+00,   0.00000000e+00,\n",
       "           6.99400000e+03]), 2),\n",
       " (array([  1.04860000e+05,   0.00000000e+00,   2.84260000e+04,\n",
       "           2.80000000e+01,   1.15000000e+03,   3.00000000e+00,\n",
       "           6.93100000e+03]), 2),\n",
       " (array([  9.65220000e+04,   0.00000000e+00,   6.11050000e+04,\n",
       "           1.90000000e+01,   0.00000000e+00,   0.00000000e+00,\n",
       "           6.92400000e+03]), 2),\n",
       " (array([  2.84950000e+04,   0.00000000e+00,   4.94420000e+04,\n",
       "           1.50000000e+01,   0.00000000e+00,   0.00000000e+00,\n",
       "           6.91200000e+03]), 2)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([  2.81430000e+04,   0.00000000e+00,   1.74000000e+02,\n",
       "           1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "           7.00000000e+03]), 3),\n",
       " (array([  1.92440000e+04,   0.00000000e+00,   2.15000000e+02,\n",
       "           2.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "           6.96800000e+03]), 3),\n",
       " (array([  4.13540000e+04,   0.00000000e+00,   4.12300000e+03,\n",
       "           4.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "           7.03400000e+03]), 3),\n",
       " (array([  1.47760000e+04,   0.00000000e+00,   5.00000000e+02,\n",
       "           1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "           6.95200000e+03]), 3),\n",
       " (array([ 16420.,      0.,      0.,      0.,      0.,      0.,   6942.]), 3)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([  4.43003000e+05,   0.00000000e+00,   1.75300000e+03,\n",
       "           4.30000000e+01,   3.85000000e+03,   1.20000000e+01,\n",
       "           6.94800000e+03]), 4),\n",
       " (array([  2.05840000e+04,   0.00000000e+00,   3.45000000e+03,\n",
       "           1.10000000e+01,   3.45000000e+03,   1.10000000e+01,\n",
       "           6.88400000e+03]), 4),\n",
       " (array([  6.03130000e+04,   0.00000000e+00,   1.00000000e+04,\n",
       "           2.60000000e+01,   3.25000000e+03,   9.00000000e+00,\n",
       "           7.82900000e+03]), 4),\n",
       " (array([  1.08137000e+05,   0.00000000e+00,   6.36800000e+03,\n",
       "           5.00000000e+00,   6.36800000e+03,   5.00000000e+00,\n",
       "           6.84400000e+03]), 4),\n",
       " (array([  5.39140000e+04,   0.00000000e+00,   3.37670000e+04,\n",
       "           4.50000000e+01,   5.55000000e+03,   2.90000000e+01,\n",
       "           6.82600000e+03]), 4)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_4.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 117260.148   5354.426  19189.607     12.426    952.164      2.885   3893.672\n",
      "1 :  36118.613     33.151   5783.135      7.065    179.646      0.553   2345.847\n",
      "2 : 139958.892     63.229  47719.475     20.822    411.513      1.235   4713.800\n",
      "3 :  59099.428     54.033   8329.658      8.975    202.061      0.626   5949.587\n",
      "4 : 192414.516    450.717  34860.233     28.069   5478.874     15.956   4650.497\n"
     ]
    }
   ],
   "source": [
    "stat_0 = Statistics.colStats(cluster_0.map(lambda x: x[0]))\n",
    "stat_1 = Statistics.colStats(cluster_1.map(lambda x: x[0]))\n",
    "stat_2 = Statistics.colStats(cluster_2.map(lambda x: x[0]))\n",
    "stat_3 = Statistics.colStats(cluster_3.map(lambda x: x[0]))\n",
    "stat_4 = Statistics.colStats(cluster_4.map(lambda x: x[0]))\n",
    "print \"0 : %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (stat_0.mean()[0],stat_0.mean()[1],stat_0.mean()[2],\n",
    "                                                            stat_0.mean()[3],stat_0.mean()[4],stat_0.mean()[5],\n",
    "                                                            stat_0.mean()[6])\n",
    "print \"1 : %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (stat_1.mean()[0],stat_1.mean()[1],stat_1.mean()[2],\n",
    "                                                            stat_1.mean()[3],stat_1.mean()[4],stat_1.mean()[5],\n",
    "                                                            stat_1.mean()[6])\n",
    "print \"2 : %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (stat_2.mean()[0],stat_2.mean()[1],stat_2.mean()[2],\n",
    "                                                            stat_2.mean()[3],stat_2.mean()[4],stat_2.mean()[5],\n",
    "                                                            stat_2.mean()[6])\n",
    "print \"3 : %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (stat_3.mean()[0],stat_3.mean()[1],stat_3.mean()[2],\n",
    "                                                            stat_3.mean()[3],stat_3.mean()[4],stat_3.mean()[5],\n",
    "                                                            stat_3.mean()[6])\n",
    "print \"4 : %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (stat_4.mean()[0],stat_4.mean()[1],stat_4.mean()[2],\n",
    "                                                            stat_4.mean()[3],stat_4.mean()[4],stat_4.mean()[5],\n",
    "                                                            stat_4.mean()[6])\n",
    "# Balance, TopStatusQualMiles, NonFlightMiles, NonFlightTrans, FlightMiles, FlightTrans, DaysSinceEnroll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "One run on Sep 27:\n",
    "0 :  37950.925     33.352   6660.214      7.644    183.511      0.567   2220.540 # Relatively new, not active\n",
    "1 :  56183.841     54.051   8370.021      8.902    205.035      0.620   5748.698\n",
    "2 : 117326.186   5445.305  19059.610     12.305    965.797      2.881   3874.831 # Top Status Qual Miles\n",
    "3 : 191736.336    471.566  33093.336     28.357   5763.133     16.769   4666.413 # Most Active\n",
    "4 : 150843.700     73.158  50474.264     21.183    473.292      1.441   4938.489 # non-flight but active customers\n",
    "````\n",
    "````\n",
    "Run 10/28/14\n",
    "0 :  38091.905     32.784   6731.402      7.630    178.718      0.555   2281.777\n",
    "1 :  57441.909     55.024   8758.131      9.104    213.633      0.646   5823.841\n",
    "2 : 191736.336    471.566  33093.336     28.357   5763.133     16.769   4666.413\n",
    "3 : 117326.186   5445.305  19059.610     12.305    965.797      2.881   3874.831\n",
    "4 : 152607.968     74.778  51066.228     21.329    478.139      1.449   4913.985\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Different runs will produce different clusters\n",
    "# Once the model is executed, the characteristics can interpreted & used in business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
